############################################################### Importing data from SQL database ##################################################################################

# timing execution
import timeit
start = timeit.default_timer()

### importing data preprocessing packages
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import pyodbc

connection_string = pyodbc.connect(DRIVER='*****',
            SERVER='*****************',
               DATABASE='**************',
               UID='*****************',
               PWD='*************')

## DimVehicleAndGroup dataset

DimVehicleAndGroup_table  = "SELECT [vehicle_Id] ,[GeneralDescription] ,[ManufacturerDescription] ,[RegisterYear] ,[VehicleModel] ,[Level 07_Descripion] FROM [dbo].[DimVehicleAndGroup]"

DimVehicleAndGroup_final = pd.read_sql(DimVehicleAndGroup_table, connection_string)

List_of_vehicle_Id = DimVehicleAndGroup_final['vehicle_Id'].tolist()


## RawDriveAccum dataset

RawDriveAccum_tabel = "SELECT cast([START_DRIVE] as date) as ORIG_TIME, *  FROM [dbo].[DriveAccumTraining]  where START_DRIVE between '20180310' and '20181101' and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860) order by START_DRIVE, VEHICLE_ID"

RawDriveAccum_final = pd.read_sql(RawDriveAccum_tabel, connection_string)

## RawEvent dataset
 
RawEvent_table = "SELECT [VEHICLE_ID] ,cast([ORIG_TIME] as date) as ORIG_TIME ,[SCHEME_ID]  FROM [dbo].[RawEvent] where ORIG_TIME between '20180310' and '20181101' and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860) order by ORIG_TIME" 

RawEvent_dataset = pd.read_sql(RawEvent_table, connection_string)

# Creating matrix with event counts for each day
RawEvent_final = RawEvent_dataset.groupby(['VEHICLE_ID', 'ORIG_TIME', 'SCHEME_ID']).size().unstack(fill_value=0)
RawEvent_final.reset_index(drop=False, inplace=True)

## RawParamInput datasets

# EngineOilPreasure features pr. day

EngineOilPressure_table_13367 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13367 ,sum(a.PARAM_VALUE) as SUM_13367 ,max(a.PARAM_VALUE) as MAX_13367 ,min(a.PARAM_VALUE) as MIN_13367 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE]  ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (13367) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOilPressure_table_13372 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13372 ,sum(a.PARAM_VALUE) as SUM_13372 ,max(a.PARAM_VALUE) as MAX_13372 ,min(a.PARAM_VALUE) as MIN_13372 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (13372) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOilPressure_table_13377 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13377 ,sum(a.PARAM_VALUE) as SUM_13377 ,max(a.PARAM_VALUE) as MAX_13377 ,min(a.PARAM_VALUE) as MIN_13377 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (13377) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOilPressure_table_13382 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13382 ,sum(a.PARAM_VALUE) as SUM_13382 ,max(a.PARAM_VALUE) as MAX_13382 ,min(a.PARAM_VALUE) as MIN_13382 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (13382) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOilPressure_table_13387 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13387 ,sum(a.PARAM_VALUE) as SUM_13387 ,max(a.PARAM_VALUE) as MAX_13387 ,min(a.PARAM_VALUE) as MIN_13387 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (13387) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOilPressure_final_13367 = pd.read_sql(EngineOilPressure_table_13367, connection_string)
EngineOilPressure_final_13372 = pd.read_sql(EngineOilPressure_table_13372, connection_string)
EngineOilPressure_final_13377 = pd.read_sql(EngineOilPressure_table_13377, connection_string)
EngineOilPressure_final_13382 = pd.read_sql(EngineOilPressure_table_13382, connection_string)
EngineOilPressure_final_13387 = pd.read_sql(EngineOilPressure_table_13387, connection_string)

# EngineOiltemperature features pr. day
EngineOiltemperature_table_13957 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13957 ,sum(a.PARAM_VALUE) as SUM_13957 ,max(a.PARAM_VALUE) as MAX_13957 ,min(a.PARAM_VALUE) as MIN_13957 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13957) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOiltemperature_table_13967 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13967 ,sum(a.PARAM_VALUE) as SUM_13967 ,max(a.PARAM_VALUE) as MAX_13967 ,min(a.PARAM_VALUE) as MIN_13967 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE]  ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13967) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOiltemperature_table_13972 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13972 ,sum(a.PARAM_VALUE) as SUM_13972 ,max(a.PARAM_VALUE) as MAX_13972 ,min(a.PARAM_VALUE) as MIN_13972 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE]  ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13972) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOiltemperature_table_13977 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13977 ,sum(a.PARAM_VALUE) as SUM_13977 ,max(a.PARAM_VALUE) as MAX_13977 ,min(a.PARAM_VALUE) as MIN_13977 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE]  ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13977) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOiltemperature_table_13982 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13982 ,sum(a.PARAM_VALUE) as SUM_13982 ,max(a.PARAM_VALUE) as MAX_13982 ,min(a.PARAM_VALUE) as MIN_13982 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE]  ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13982) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

EngineOiltemperature_table_13957 = pd.read_sql(EngineOiltemperature_table_13957, connection_string)
EngineOiltemperature_table_13967 = pd.read_sql(EngineOiltemperature_table_13967, connection_string)
EngineOiltemperature_table_13972 = pd.read_sql(EngineOiltemperature_table_13972, connection_string)
EngineOiltemperature_table_13977 = pd.read_sql(EngineOiltemperature_table_13977, connection_string)
EngineOiltemperature_table_13982 = pd.read_sql(EngineOiltemperature_table_13982, connection_string)

# Transmission oil temperature
TransmissionOilTemperature_table_13317 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13317 ,sum(a.PARAM_VALUE) as SUM_13317 ,max(a.PARAM_VALUE) as MAX_13317 ,min(a.PARAM_VALUE) as MIN_13317 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13317) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

TransmissionOilTemperature_table_13322 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13322 ,sum(a.PARAM_VALUE) as SUM_13322 ,max(a.PARAM_VALUE) as MAX_13322 ,min(a.PARAM_VALUE) as MIN_13322 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13322) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

TransmissionOilTemperature_final_13317 = pd.read_sql(TransmissionOilTemperature_table_13317, connection_string)
TransmissionOilTemperature_final_13322 = pd.read_sql(TransmissionOilTemperature_table_13322, connection_string)

## HYD AIR intake
HYD_AIR_intake_table_13342 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13342 ,sum(a.PARAM_VALUE) as SUM_13342 ,max(a.PARAM_VALUE) as MAX_13342 ,min(a.PARAM_VALUE) as MIN_13342 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13342) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

HYD_AIR_intake_table_13347 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13347 ,sum(a.PARAM_VALUE) as SUM_13347 ,max(a.PARAM_VALUE) as MAX_13347 ,min(a.PARAM_VALUE) as MIN_13347 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13347) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

HYD_AIR_intake_table_13352 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13352 ,sum(a.PARAM_VALUE) as SUM_13352 ,max(a.PARAM_VALUE) as MAX_13352 ,min(a.PARAM_VALUE) as MIN_13352 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13352) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

HYD_AIR_intake_table_13357 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_13357 ,sum(a.PARAM_VALUE) as SUM_13357 ,max(a.PARAM_VALUE) as MAX_13357 ,min(a.PARAM_VALUE) as MIN_13357 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181210' and PARAM_TYPE in (13357) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"


HYD_AIR_intake_final_13342 = pd.read_sql(HYD_AIR_intake_table_13342, connection_string)
HYD_AIR_intake_final_13347 = pd.read_sql(HYD_AIR_intake_table_13347, connection_string)
HYD_AIR_intake_final_13352 = pd.read_sql(HYD_AIR_intake_table_13352, connection_string)
HYD_AIR_intake_final_13357 = pd.read_sql(HYD_AIR_intake_table_13357, connection_string)

# Coolant_temperature features pr. day
Coolant_temperature_table_18687 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_18687 ,sum(a.PARAM_VALUE) as SUM_18687 ,max(a.PARAM_VALUE) as MAX_18687 ,min(a.PARAM_VALUE) as MIN_18687 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (18687) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

Coolant_temperature_table_18692 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_18692 ,sum(a.PARAM_VALUE) as SUM_18692 ,max(a.PARAM_VALUE) as MAX_18692 ,min(a.PARAM_VALUE) as MIN_18692 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (18692) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

Coolant_temperature_table_18697 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_18697 ,sum(a.PARAM_VALUE) as SUM_18697 ,max(a.PARAM_VALUE) as MAX_18697 ,min(a.PARAM_VALUE) as MIN_18697 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (18697) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

Coolant_temperature_table_18702 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_18702 ,sum(a.PARAM_VALUE) as SUM_18702 ,max(a.PARAM_VALUE) as MAX_18702 ,min(a.PARAM_VALUE) as MIN_18702 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (18702) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

Coolant_temperature_table_18707 = "select VEHICLE_ID ,CAST(a.ORIG_TIME as date) as ORIG_TIME ,avg(a.PARAM_VALUE) as AVG_18707 ,sum(a.PARAM_VALUE) as SUM_18707 ,max(a.PARAM_VALUE) as MAX_18707 ,min(a.PARAM_VALUE) as MIN_18707 from (SELECT [VEHICLE_ID] ,[PARAM_TYPE] ,case when [PARAM_VALUE] >  lead(param_value) over (partition by [VEHICLE_ID],[PARAM_TYPE] order by Orig_time) then [PARAM_VALUE] else 0 end as [PARAM_VALUE] ,[ORIG_TIME] FROM [dbo].[RawParamInput] where ORIG_TIME between '20180310' and '20181101' and PARAM_TYPE in (18707) and VEHICLE_ID in (142982, 145343, 142803, 145323, 142977, 172868, 142983, 145316, 155554, 172974, 142986, 151290, 145329, 151289, 155568, 145322, 145307, 145324, 145332, 172865, 167518, 172970, 145345, 145337, 145359, 145328, 172867, 172869, 145330, 155556, 142807, 172862, 167519, 142811, 151294, 146372, 142800, 145338, 167517, 172873, 145363, 145366, 145320, 145314, 145365, 172871, 172874, 74277, 151288, 145340, 155441, 172872, 145319, 145321, 145352, 151292, 145304, 172979, 145313, 172875, 172858, 145333, 172981, 172863, 151291, 142991, 155562, 172861, 145339, 145326, 145341, 145306, 145336, 172877, 76844, 145351, 146385, 142984, 145335, 145361, 172878, 145342, 145364, 145346, 145317, 142797, 145308, 172094, 172980, 143006, 145310, 151293, 172975, 142812, 145334, 172876, 155561, 145315, 145355, 172864, 145327, 146186, 172870, 172879, 155567, 167516, 172866, 172859, 145367, 145305, 172860)) a where a.PARAM_VALUE > 0 group by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date) order by VEHICLE_ID, PARAM_TYPE, CAST(ORIG_TIME as date)"

Coolant_temperature_final_18687 = pd.read_sql(Coolant_temperature_table_18687, connection_string)
Coolant_temperature_final_18692 = pd.read_sql(Coolant_temperature_table_18692, connection_string)
Coolant_temperature_final_18697 = pd.read_sql(Coolant_temperature_table_18697, connection_string)
Coolant_temperature_final_18702 = pd.read_sql(Coolant_temperature_table_18702, connection_string)
Coolant_temperature_final_18707 = pd.read_sql(Coolant_temperature_table_18707, connection_string)

connection_string.close()

# Creating logic dataframe Vehicles, dates
vehicles = List_of_vehicle_Id
days = pd.date_range(start=RawDriveAccum_final['ORIG_TIME'].min(), end=RawDriveAccum_final['ORIG_TIME'].max(), freq = 'D') 

logicDataframe = pd.MultiIndex.from_product([vehicles, days], names = ["VEHICLE_ID", "ORIG_TIME"])
logicDataframe = pd.DataFrame(index = logicDataframe).reset_index()



# Merging datasets together to one dataframe for ML

RawDriveAccum_DimVehicleAndGroup_final = pd.merge(RawDriveAccum_final, DimVehicleAndGroup_final, left_on='VEHICLE_ID', right_on = 'vehicle_Id' , how = 'left')
RawDriveAccum_DimVehicleAndGroup_final = RawDriveAccum_DimVehicleAndGroup_final.drop(['vehicle_Id'], axis = 1)

RawDriveAccum_DimVehicleAndGroup_final['ORIG_TIME'] = pd.to_datetime(RawDriveAccum_DimVehicleAndGroup_final['ORIG_TIME'])
dataset = pd.merge(logicDataframe, RawDriveAccum_DimVehicleAndGroup_final, left_on=['ORIG_TIME', 'VEHICLE_ID'], right_on=['ORIG_TIME', 'VEHICLE_ID'], how = 'left')

RawEvent_final['ORIG_TIME'] = pd.to_datetime(RawEvent_final['ORIG_TIME'])
dataset  = pd.merge(dataset, RawEvent_final, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

# EngineOilPressure & Coolant_temperature & EngineOilTemperature & TransOiltemp # others

EngineOilPressure_final  = pd.merge(EngineOilPressure_final_13367, EngineOilPressure_final_13372, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_final  = pd.merge(EngineOilPressure_final, EngineOilPressure_final_13377, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_final  = pd.merge(EngineOilPressure_final, EngineOilPressure_final_13382, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_final  = pd.merge(EngineOilPressure_final, EngineOilPressure_final_13387, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

EngineOilPressure_Coolant_temperature_final = pd.merge(EngineOilPressure_final, Coolant_temperature_final_18687, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_final = pd.merge(EngineOilPressure_Coolant_temperature_final, Coolant_temperature_final_18692, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_final = pd.merge(EngineOilPressure_Coolant_temperature_final, Coolant_temperature_final_18697, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_final = pd.merge(EngineOilPressure_Coolant_temperature_final, Coolant_temperature_final_18702, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_final = pd.merge(EngineOilPressure_Coolant_temperature_final, Coolant_temperature_final_18707, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

EngineOilPressure_Coolant_temperature_Oil_Pressure_final = pd.merge(EngineOilPressure_Coolant_temperature_final, EngineOiltemperature_table_13957, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_final, EngineOiltemperature_table_13967, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_final, EngineOiltemperature_table_13972, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_final, EngineOiltemperature_table_13977, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_final, EngineOiltemperature_table_13982, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_final, TransmissionOilTemperature_final_13317, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_final, TransmissionOilTemperature_final_13322, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_final, HYD_AIR_intake_final_13342, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, HYD_AIR_intake_final_13347, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, HYD_AIR_intake_final_13352, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, HYD_AIR_intake_final_13357, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, EstimatedEngineParasiticLosses_final_7953, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, NextOilChangeInKms_final_34143, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, DaysSinceLastService_final_34385, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = pd.merge(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, NextServiceCallInDays_final_34846, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')

# Imputating 0 for NaN values as NaN means that it that the temperature / pressure is not within the specific interval
EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final = EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final.fillna(0)

EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final['ORIG_TIME'] = pd.to_datetime(EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final['ORIG_TIME'])
dataset = pd.merge(dataset, EngineOilPressure_Coolant_temperature_Oil_Pressure_TransOilTemp_Others_AirIntake__final, left_on=['VEHICLE_ID', 'ORIG_TIME'], right_on=['VEHICLE_ID','ORIG_TIME'], how = 'left')
dataset = dataset.drop(['START_DRIVE'], axis = 1)






############################################################### Creating target variables ##################################################################################

 #121497 - Engine fault

#### Creating a new column as the target variable
def target_f(row):
    if row[121497] >= 1:
        val = '1' # target - positive results
    else:
        val = '0' # negative results
    return val

dataset['Target'] = dataset.apply(target_f, axis=1) # looping over all rows (columns)

dataset['Target'].value_counts()

dataset = dataset.drop([121497], axis = 1)
dataset.shape


#### calculating the amoung of days until next fault pr. vehicle (RUL)

dataset['colFromIndex'] = dataset.index
dataset = dataset.sort_values(by=['colFromIndex'], ascending = False)

def count_consecutive_items_n_cols(df, col_name_list, output_col):     
    cum_sum_list = [
        (df[col_name] != df[col_name].shift(1)).cumsum().tolist() for col_name in col_name_list
    ]
    df[output_col] = df.groupby(
        ["_".join(map(str, x)) for x in zip(*cum_sum_list)]
    ).cumcount() + 1
    return df

list_labels = ['Target', 'VEHICLE_ID']

count_consecutive_items_n_cols(dataset, list_labels, 'count')


# inkluding a 0 for every day where the fault happens

def target_c(row):
    if row['Target'] == '1' and row['count'] == 1:
        val = 0
    elif row['Target'] == '1' and row['count'] == 2: # sometimes fault apear 2 days in a row
        val = 0
    else:
        val = row['count']
    return val

dataset['RUL'] = dataset.apply(target_c, axis=1) # looping over all rows (columns)
dataset = dataset.drop(['count'], axis = 1)


# generate binary classification label if the vehicle is going to break within the next 15, 20 and 30 days
def target_t14(row):
    if row ['RUL'] <= 14:
        val = 1
    else:
        val = 0
    return val

def target_t20(row):
    if row ['RUL'] <= 20:
        val = 1
    else:
        val = 0
    return val

def target_t30(row):
    if row ['RUL'] <= 30:
        val = 1
    else:
        val = 0
    return val

dataset['target_14'] = dataset.apply(target_t14, axis=1) # looping over all rows (columns)
dataset['target_20'] = dataset.apply(target_t20, axis=1) # looping over all rows (columns)
dataset['target_30'] = dataset.apply(target_t30, axis=1) # looping over all rows (columns)

# last sort the values in the correct order
dataset = dataset.sort_values(by=['colFromIndex'])

## only choose index of vehicles with RUL = 0, meaning actually leads to a fault
dataset_LARS = dataset.loc[:, ['VEHICLE_ID', 'ORIG_TIME', 'RUL', 'Target']]
index_list_RUL_ZERO = dataset_LARS.index[dataset_LARS['RUL'] == 0].tolist()
dataset_LARS = dataset_LARS.iloc[index_list_RUL_ZERO, :]
dataset_LARS = dataset_LARS.reset_index()

# we have multiple of the same vehicles, so we only choose the first one in the cycle
index_RUL_ZERO = dataset_LARS.groupby('VEHICLE_ID').first()
# only select the index for each vehicle where the RUL is 0
index_RUL_ZERO_final = index_RUL_ZERO.loc[:, 'index'].tolist()


# filter dataset to only include vehicles with the fault.
# first make a list of vehicles with the fault
Vehicle_faults = index_RUL_ZERO.reset_index()['VEHICLE_ID'].tolist()
# filter rows in dataset by vehicle from list
dataset_unique = dataset.loc[dataset['VEHICLE_ID'].isin(Vehicle_faults)]

# Identify the first row (index) of each vehicle group
first_index_vehicle = dataset_unique.reset_index().groupby('VEHICLE_ID').first()['index'].tolist()

## select only the subset of the dataset
dataset_final = pd.DataFrame()
vehicle_1 = pd.DataFrame()

len(first_index_vehicle) == len(index_RUL_ZERO_final)

for x, y in zip(first_index_vehicle, index_RUL_ZERO_final):    
    vehicle_1 = dataset.iloc[x:y+1, :] 
    dataset_final = pd.concat([dataset_final,vehicle_1])
    

######### nth fault identification pr. vehicle

nth__1 = [0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19,20]
nth_2 =  [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]
ID = ['2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22']
for x, y, z in zip(nth__1, nth_2, ID):
    ######### nth fault identification
    index_RUL_ZERO = dataset_LARS.groupby('VEHICLE_ID').nth(x) #<---- CHANGE HERE
    index_RUL_ZERO_2 = dataset_LARS.groupby('VEHICLE_ID').nth(y) #<---- CHANGE HERE
    # only select the index for each vehicle where the RUL is 0
    index_RUL_ZERO_final_2 = index_RUL_ZERO_2.loc[:, 'index'].tolist()
    
    # get the ID of each vehicle
    Vehicle_faults2 = index_RUL_ZERO_2.reset_index()['VEHICLE_ID'].tolist()
    
    index_RUL_ZERO_index = index_RUL_ZERO.reset_index()
    # filter dataset with first faults pr. vehicle 
    dataset_unique2 = index_RUL_ZERO_index.loc[index_RUL_ZERO_index['VEHICLE_ID'].isin(Vehicle_faults2)]
    
    
    # Identify the first row (index) of each vehicle group
    second_index_vehicle = dataset_unique2.reset_index().groupby('VEHICLE_ID').nth(0)['index'].tolist()
    
    # select the subset of the dataset for 2. fault
    len(second_index_vehicle) == len(index_RUL_ZERO_final_2)
    
    ## select only the subset of the dataset
    dataset_final2 = pd.DataFrame()
    vehicle_2 = pd.DataFrame()
    
    for x, y in zip(second_index_vehicle, index_RUL_ZERO_final_2):    
        vehicle_2 = dataset.iloc[x+1:y+1, :] 
        dataset_final2 = pd.concat([dataset_final2,vehicle_2])
    
    ### RUL needs to be more than 14    
    threshold_14 = dataset_final2.groupby('VEHICLE_ID').max()
    dataset_final2_vehicles = threshold_14[threshold_14['RUL'] >= 14]
    # filter out vehicles with max rull less than 14
    dataset_final2_vehicles = dataset_final2_vehicles.reset_index()['VEHICLE_ID'].tolist()
    dataset_final2_final = dataset_final2.loc[dataset_final2['VEHICLE_ID'].isin(dataset_final2_vehicles)]
      
    # add 002 to the vehicle numbers, so we know it is the second fault of the vehicle
    dataset_final2_final['VEHICLE_ID'] = dataset_final2_final['VEHICLE_ID'].astype(str)
    dataset_final2_final = dataset_final2_final.assign(VEHICLE_ID = dataset_final2_final['VEHICLE_ID'].astype(str) + '0' + z) #<----- CHANGE HERE
    # convert to int again to be able to cbind
    dataset_final2_final['VEHICLE_ID'] = dataset_final2_final['VEHICLE_ID'].astype(int)
    
    ## merge(column bind) with first fault dataset
    dataset_final = pd.concat([dataset_final, pd.DataFrame(dataset_final2_final)])


## Final dataset
dataset = dataset_final   


columnnames = dataset_final.columns.tolist()

############################################################### Creating cycle variable ##################################################################################

### Converting Datetime to integer cycle to incklude the time interval in the data mining model
from datetime import datetime

days2 = pd.date_range(dataset['ORIG_TIME'].min(), dataset['ORIG_TIME'].max())
df = pd.DataFrame(days2.date)
df['cycle_day']=df.reset_index().index
df[0] =  pd.to_datetime(df[0])

dataset = pd.merge(dataset, df, left_on=['ORIG_TIME'], right_on=[0], how = 'left')
dataset = dataset.drop([0], axis = 1)

# dropping ealier target variables
dataset = dataset.drop(['Target'], axis = 1)
dataset = dataset.drop(['RUL'], axis = 1)


stop = timeit.default_timer()

print('Time: ', stop - start)

#------------------------------------------------------------ Importing of data done ---------------------------------------------------------------------

#datasetbackup = dataset

dataset = datasetbackup


#### Choose here what target variable we are going with

dataset['target'] = dataset['target_14'] 
#dataset['target'] = dataset['target_20'] 
#dataset['target'] = dataset['target_30'] 

# dropping the other variabels
dataset = dataset.drop(['target_14'], axis = 1)
dataset = dataset.drop(['target_20'], axis = 1)
dataset = dataset.drop(['target_30'], axis = 1)



############################################################### Handling Missing values, outliers and dublicates ##################################################################################

#### checking overall desciptive statisitics of dataset
dataset.shape # how many rows and columns
dataset.info() # missing values and type of columns, and how many of differet types of columns
dataset.describe() # statistics of columns
dataset.dtypes # dtypes of features


###### removing dublicates ######
dataset = dataset.drop_duplicates() # no dublicates

###### checking and handling outliers ##### - Source: https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba 

#### Detecting outlier

# 1. Box plot and scatter plots 
# see scatter plot in explorative data analysis

# 2. Using mathematical functions

# 2.1. Z - score (3 or -3 rule)
log_list = [col for col in dataset.columns if col not in ['VEHICLE_ID', 'ORIG_TIME', 'START_LOCATION', 'END_LOCATION', 'cycle_day', 'GeneralDescription', 'ManufacturerDescription', 'RegisterYear', 'VehicleModel', 'Level 07_Descripion', 'Day', 'DayOfweek', 'WeekOfYear', 'Month', 'Quarter', 'Year', 'Weekend', 'target', 'SumEngineHours', 'AvgEnginehours']]

from scipy import stats
import numpy as np
z = np.abs(stats.zscore(dataset[log_list]))
print(z)

# rule
#threshold = 3 and -3
outliers_zscore = pd.DataFrame((np.abs(stats.zscore(dataset[log_list])) > 3) | (np.abs(stats.zscore(dataset[log_list])) < -3)).sum(axis = 1)
outliers_zscore = pd.DataFrame(outliers_zscore)
outliers_zscore[0] = np.where(outliers_zscore[0] > 10, -1, 1)


# 2.2  IQR score - measure of dispersion similar to sd but more robust to outliers
# calculate IQR for each column
Q1 = dataset[log_list].quantile(0.25)
Q3 = dataset[log_list].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

# True values equal outliers
outliers_IQR = pd.DataFrame(((dataset[log_list] < (Q1 - 1.5 * IQR)) |(dataset[log_list] > (Q3 + 1.5 * IQR))).sum(axis = 1))

outliers_IQR[0] = np.where(outliers_IQR[0] > 10, -1, 1)

# 3. detecting outliers (rows) with Isolation Forest algorithm
from sklearn.utils.class_weight import compute_sample_weight
class_weight = compute_sample_weight(class_weight ='balanced', y = dataset['target'])

from sklearn.ensemble import IsolationForest
classifier_outlier = IsolationForest(max_samples=100, random_state=0, contamination = 0.05, n_jobs = -1)
classifier_outlier = classifier_outlier.fit(dataset[log_list], sample_weight = class_weight)
outliers_forest = classifier_outlier.predict(dataset[log_list])

# 4. Detecting outlier (rows) with EllipticEvelope
from sklearn.covariance import EllipticEnvelope

outlier_detector_ellip = EllipticEnvelope(contamination=0.05, random_state = 0)
outlier_detector_ellip.fit(dataset[log_list])
outliers_ellip = outlier_detector_ellip.predict(dataset[log_list])

#outliers_ellip = outlier_detector_ellip.fit_predict(x_train_res) # same as fit, just without the possibility to fit 
from sklearn.neighbors import LocalOutlierFactor

# KNN outlier (rows) detection
outlier_detector_KNN = LocalOutlierFactor(n_neighbors=20, metric='minkowski', contamination=0.05, n_jobs= -1)
outlier_detector_KNN = outlier_detector_KNN.fit(dataset[log_list])
outliers_KNN =  outlier_detector_KNN.fit_predict(dataset[log_list]) # Returns -1 for anomalies/outliers and +1 for inliers. 

outliers_rows = pd.DataFrame()
outliers_list = [outliers_zscore, outliers_IQR, outliers_forest, outliers_ellip, outliers_KNN]
for i in outliers_list:
    outliers_rows = pd.concat([outliers_rows.reset_index(drop=True), pd.DataFrame(i)], axis=1)

outliers_rows.columns = ['outliers_zscore', 'outliers_IQR', 'outliers_forest', 'outliers_ellip', 'outliers_KNN']

outliers_rows['outliers'] = outliers_rows.sum(axis = 1) # negative values are outliers (3 or more methods define row as outliers)

outliers_list_rows = outliers_rows.index[outliers_rows['outliers'] < 0].tolist




#### Handling outliers

# 1. drop outliers

#1.1 based on Z score
dataset = dataset[(dataset['outliers'] < 3).all(axis=1)]
dataset.shape

#1.1 based on IQR score
dataset = dataset[~((dataset < (Q1 - 1.5 * IQR)) |(dataset > (Q3 + 1.5 * IQR))).any(axis=1)]
dataset.shape

# 2. imputing outliers

# 3. Mark to see if they have an effect 
# Create feature based on boolean condition
dataset['outlier'] = np.where(dataset['outlier'] < 20, 0,) 

# 4. Rescale with log so outliers have a not as great effect
dataset['outlier'] = [np.log(x) for x in dataset['outlier']]






################################## checking and handling missing values ##### 


#### checking for missing values (visualization)

sns.set_style(style='white', rc=None)
dataset.isnull().sum(axis=0).plot(kind='bar',
                              figsize=(15,10)) # plotting the counts of features

dataset.isna().sum(axis=0)
missing_col = dataset.isnull().sum(axis=0)

dataset.isna().any()
dataset.columns[dataset.isna().any()].tolist()
dataset.columns[dataset.isnull().any()].tolist()



# 1.1 removing Rows if all values are missing
dataset = dataset.dropna(axis=0, how='all')

# removing all rows where all observatiosn are missing as the vehicle was not driving, as there were no events.
index_na = np.where(dataset['START_LOCATION'].isna())[0]
dataset = dataset.drop(dataset.index[index_na])

missing_col = pd.DataFrame(dataset.isnull().sum(axis=0))


# 1.2 - Remove coulumns if more than 50% missing 

missing_col['out'] = missing_col / len(dataset)
missing_col[(missing_col['out'] >= 0.50)]

dataset = dataset.drop(['AvgEnginehours'], axis = 1) # change manually
dataset = dataset.drop(['SumEngineHours'], axis = 1) # change manually
#dataset = dataset.dropna(axis=1, how='all')
#dataset = dataset.dropna(axis=1, how='any')

missing_col = pd.DataFrame(dataset.isnull().sum(axis=0))



# 2.2 missing values imputation with mean on whole dataset - Source: #https://towardsdatascience.com/the-tale-of-missing-values-in-python-c96beb0e8a9d
from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0) # missing values are called NaN in the dataset
imputer = imputer.fit(dataset.loc[:, ['AvgDriveFuelScore','AvgIdleEventDuration', 'AvgTimeFromPrevDrive', 'AvgMileage']])
dataset.loc[:, ['AvgDriveFuelScore','AvgIdleEventDuration', 'AvgTimeFromPrevDrive', 'AvgMileage']] = imputer.transform(dataset.loc[:, ['AvgDriveFuelScore','AvgIdleEventDuration', 'AvgTimeFromPrevDrive', 'AvgMileage']])  # indicate which indices the features of interest have

# use for integers columns (categorical variables)
imputer_median = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) # missing values are called NaN in the dataset
imputer_median = imputer_median.fit(dataset.loc[:, ['SumDriveFuelScore','SumIdleEventDuration','SumTimeFromPrevDrive', 'SumMileage', 'MaxDriverGrade']])
dataset.loc[:, ['SumDriveFuelScore','SumIdleEventDuration','SumTimeFromPrevDrive', 'SumMileage', 'MaxDriverGrade']] = imputer_median.transform(dataset.loc[:, ['SumDriveFuelScore','SumIdleEventDuration','SumTimeFromPrevDrive', 'SumMileage', 'MaxDriverGrade']])  # indicate which indices the features of interest have



# 2.2 imputating missing values as 0 for sensor values where 0 means not in that current state
missing_col = pd.DataFrame(dataset.isnull().sum(axis=0))
missing_col = missing_col.index[missing_col[0] > 0].tolist()
dataset[missing_col] = dataset[missing_col].fillna(value=0)

# final check if we have more missing values
dataset.columns[dataset.isna().any()].tolist()

#### checking categorical feateures for missing values ####


# counting number of observations of each class in categorical variables
dataset['GeneralDescription'].isnull().sum(axis=0) # any missing values?
dataset['GeneralDescription'].value_counts() # how many categories?

dataset['ManufacturerDescription'].isnull().sum(axis=0)
dataset['ManufacturerDescription'].value_counts()

dataset['RegisterYear'].isnull().sum(axis=0)
dataset['RegisterYear'].value_counts()

dataset['VehicleModel'].isnull().sum(axis=0)
dataset['VehicleModel'].value_counts()

dataset['Level 07_Descripion'].isnull().sum(axis=0)
dataset['Level 07_Descripion'].value_counts()

dataset['START_LOCATION'].isnull().sum(axis=0)
dataset['START_LOCATION'].value_counts() # maybe Bucket?

dataset['END_LOCATION'].isnull().sum(axis=0)
dataset['END_LOCATION'].value_counts() # maybe bucket?

# plotting the frequencies
ax = dataset['GeneralDescription'].value_counts().plot(kind='bar',
                                    title="Counts of Events",
                                    figsize=(20,20))
ax.set_xlabel("GeneralDescription")
ax.set_ylabel("Frequency")

# final check if we have more missing values
dataset.columns[dataset.isna().any()].tolist()

#------------------------------------------------------------ Handling of missing values and outliers done ---------------------------------------------------------------------


############################################################### Eksplorative data analyse ##################################################################################


#### checking overall desciptive statisitics of dataset
dataset.shape # how many rows and columns
dataset.info() # missing values and type of columns
dataset.describe() # statistics of columns
dataset.dtypes # dtypes of features

################################### Analyse of target varible

#### chekcing distribution of target variable
target_dis = dataset['target'].value_counts()
dis = target_dis[1] / len(dataset)
dis

sns.set_style("whitegrid")

ax = dataset['target'].value_counts().plot(kind='bar',
                                    title="Counts of Events",
                                    figsize=(10,10))
ax.set_xlabel("Class")
ax.set_ylabel("Frequency")
print('% of class 1:', dis)


###########


###################### Investigating distributions of the dataset ##############################

dataset.dtypes

D1 = dataset.loc[:, ['SumMileage', 'AvgMileage', 'SumDriveDuration', 'AvgDriveDuration', 'SumIdleDuration', 'AvgIdleDuration', 'target']] # continuous
D2 = dataset.loc[:, ['SumFuelUsed', 'AvgFuelUsed', 'SumIdleEventDuration', 'AvgIdleEventDuration', 'SumDoaDuration', 'AvgDoaDuration', 'target']] # categorical variables
#D3 = dataset.loc[:, ['SumDoaMileage', 'AvgDoamileage', 'SumDriveFuelScore', 'AvgDriveFuelScore', 'SumTimeFromPrevDrive', 'AvgTimeFromPrevDrive', 'target']] # categorical variables
D4 = dataset.loc[:, ['AVG_13367', 'SUM_13367', 'MAX_13367', 'MIN_13367', 'target']] # categorical transformed to counted continoues varibles
D5 = dataset.loc[:, ['AVG_13372', 'SUM_13372', 'MAX_13372', 'MIN_13372', 'target']] # categorical transformed to counted continoues varibles
D6 = dataset.loc[:, ['AVG_13377', 'SUM_13377', 'MAX_13377', 'MIN_13377', 'target']] # categorical transformed to counted continoues varibles
D7 = dataset.loc[:, ['AVG_13382', 'SUM_13382', 'MAX_13382', 'MIN_13382', 'target']] # categorical transformed to counted continoues varibles
D8 = dataset.loc[:, ['AVG_13387', 'SUM_13387', 'MAX_13387', 'MIN_13387', 'target']] # categorical transformed to counted continoues varibles
D9 = dataset.loc[:, ['AVG_18687', 'SUM_18687', 'MAX_18687', 'MIN_18687', 'target']] # categorical transformed to counted continoues varibles
D10 = dataset.loc[:, ['AVG_18692', 'SUM_18692', 'MAX_18692', 'MIN_18692', 'target']] # categorical transformed to counted continoues varibles
D11 = dataset.loc[:, ['AVG_18697', 'SUM_18697', 'MAX_18697', 'MIN_18697', 'target']] # categorical transformed to counted continoues varibles
D12 = dataset.loc[:, ['AVG_18702', 'SUM_18702', 'MAX_18702', 'MIN_18702', 'target']] # categorical transformed to counted continoues varibles
D13 = dataset.loc[:, ['AVG_18707', 'SUM_18707', 'MAX_18707', 'MIN_18707', 'target']] # categorical transformed to counted continoues varibles
D14 = dataset.loc[:, ['MaxDriverGrade', 'target']] # categorical transformed to counted continoues varibles


#### Checking distribution of continoues variables 

# - ## Option 1.1 Scatter Matrix:
import seaborn as sns

pd.scatter_matrix(D1, alpha = 0.3, figsize = (15,10), diagonal = 'kde'); # some are right skewed, log transform all of them
pd.scatter_matrix(D2, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # log transform all of them
#pd.scatter_matrix(D3, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
pd.scatter_matrix(D4, alpha = 0.3, figsize = (14,8), diagonal = 'kde');
pd.scatter_matrix(D5, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # log transform all of them
pd.scatter_matrix(D6, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # log transform all of them
pd.scatter_matrix(D7, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # log transform all of them
pd.scatter_matrix(D8, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # only few observations, - log transform anyway
pd.scatter_matrix(D9, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # Log transform all of them 
pd.scatter_matrix(D10, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # Log transform all of them 
pd.scatter_matrix(D11, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # only few observations - log transform anyway
pd.scatter_matrix(D12, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # only new observations - log transform anyway
pd.scatter_matrix(D13, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # only new observations - log transform anyway
pd.scatter_matrix(D14, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); # log transform anyway


# option 1.2
sns.pairplot(D1)
sns.pairplot(D2)



##### Checking relationship between features and target variable

df_t = dataset['target']
dataset.columns.get_loc("target") # 218
len(dataset.columns) # 219

#### Correlation between independent and target variables
C1 = dataset.iloc[:, [2,3,4,5,6,7,8,9,10,11,12,223]]
C2 = dataset.iloc[:, [13,14,15,16,17,18,19,20,21,22,23,223]]
C3 = dataset.iloc[:, [24,25,26,27,28,29,30,31,32,33,34,223]]
C4 = dataset.iloc[:, [35,36,37,38,39,40,41,42,43,44,45,223]]
C5 = dataset.iloc[:, [46,47,48,49,50,51,52,53,54,55,56,223]]
C6 = dataset.iloc[:, [57,58,59,60,61,62,63,64,65,66,67,223]]
C7 = dataset.iloc[:, [68,69,70,71,72,73,74,75,76,77,78,223]]
C8 = dataset.iloc[:, [79,80,81,82,83,84,85,86,87,88,89,223]]
C9 = dataset.iloc[:, [90,91,92,93,94,95,96,97,98,99,100,223]]


C10 = dataset.iloc[:, 100:110]
C10 = pd.concat([df_t.reset_index(drop=True), C10], axis=1)
C11 = dataset.iloc[:, 110:120]
C11 = pd.concat([df_t.reset_index(drop=True), C11], axis=1)
C12 = dataset.iloc[:, 120:130]
C12 = pd.concat([df_t.reset_index(drop=True), C12], axis=1)
C13 = dataset.iloc[:, 130:140]
C13 = pd.concat([df_t.reset_index(drop=True), C13], axis=1)
C14 = dataset.iloc[:, 140:150]
C14 = pd.concat([df_t.reset_index(drop=True), C14], axis=1)


C15 = dataset.iloc[:, 150:160]
C15 = pd.concat([df_t.reset_index(drop=True), C15], axis=1)
C16 = dataset.iloc[:, 160:170]
C16 = pd.concat([df_t.reset_index(drop=True), C16], axis=1)
C17 = dataset.iloc[:, 170:180]
C17 = pd.concat([df_t.reset_index(drop=True), C17], axis=1)
C18 = dataset.iloc[:, 180:190]
C18 = pd.concat([df_t.reset_index(drop=True), C18], axis=1)
C19 = dataset.iloc[:, 190:200]
C19 = pd.concat([df_t.reset_index(drop=True), C19], axis=1)

C20 = dataset.iloc[:, 200:210]
C20 = pd.concat([df_t.reset_index(drop=True), C20], axis=1)
C21 = dataset.iloc[:, 210:220]
C21 = pd.concat([df_t.reset_index(drop=True), C21], axis=1)
C22 = dataset.iloc[:, 220:230]
C22 = pd.concat([df_t.reset_index(drop=True), C22], axis=1)
C23 = dataset.iloc[:, 230:240]
C23 = pd.concat([df_t.reset_index(drop=True), C23], axis=1)
C24 = dataset.iloc[:, 240:250]
C24 = pd.concat([df_t.reset_index(drop=True), C24], axis=1)

C25 = dataset.iloc[:, 260:270]
C25 = pd.concat([df_t.reset_index(drop=True), C25], axis=1)
C26 = dataset.iloc[:, 270:280]
C26 = pd.concat([df_t.reset_index(drop=True), C26], axis=1)
C27 = dataset.iloc[:, 280:290]
C27 = pd.concat([df_t.reset_index(drop=True), C27], axis=1)
C28 = dataset.iloc[:, 290:300]
C28 = pd.concat([df_t.reset_index(drop=True), C28], axis=1)
C29 = dataset.iloc[:, 300:310]
C29 = pd.concat([df_t.reset_index(drop=True), C29], axis=1)
C30 = dataset.iloc[:, 310:323]
C30 = pd.concat([df_t.reset_index(drop=True), C30], axis=1)


##### correlation matrix - Check relationships between variables

# Option 1 - Correlation matrix with numbers 
#source: https://www.kaggle.com/arthurtok/feature-ranking-rfe-random-forest-linear-models

def Corr_matrix(Data_C):
    str_list = [] # empty list to contain columns with strings (words)
    for colname, colvalue in Data_C.iteritems():
        if type(colvalue[1]) == str:
            str_list.append(colname)
# Get to the numeric columns by inversion            
    num_list = Data_C.columns.difference(str_list) 
# Create Dataframe containing only numerical features
    C1_num = Data_C[num_list]
    f, ax = plt.subplots(figsize=(16, 12))
    plt.title('Pearson Correlation of features')
# Draw the heatmap using seaborn
#sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="PuBuGn", linecolor='k', annot=True)
    Corr_map = sns.heatmap(C1_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="cubehelix", linecolor='k', annot=True)
    return

Corr_matrix(C1) # o.15
Corr_matrix(C2) # 0.056
Corr_matrix(C3) # non
Corr_matrix(C4) # -0.18
Corr_matrix(C5) # -0.11
Corr_matrix(C6) # 0.18
Corr_matrix(C7) # 0.3 - 119016
Corr_matrix(C8) #-0.16
Corr_matrix(C9) # -0.16
Corr_matrix(C10) # -0.19
Corr_matrix(C11) # 0.17
Corr_matrix(C12) # 0.18 , 0.22
Corr_matrix(C13) # 0.21 , 0.21
Corr_matrix(C14) # 0.19
Corr_matrix(C15) # 0.17
Corr_matrix(C16) # 0.17
Corr_matrix(C17) # -0.11
Corr_matrix(C18) # 0.14
Corr_matrix(C19) # 0.13
Corr_matrix(C20) # 0.12
Corr_matrix(C21) # 0.18, 0.17, 0.16
Corr_matrix(C22) # 0.15
Corr_matrix(C23) # -0.15
Corr_matrix(C24) # -0.11
Corr_matrix(C25) # -0.11
Corr_matrix(C26) # 0.12
Corr_matrix(C27) # -0.9
Corr_matrix(C28) # -0.11
Corr_matrix(C29) # 0.0057
Corr_matrix(C30) # 0.14



#---------------------------------------------------------------- Eksplorative data analyse done -------------------------------------------------------------------------------------

############################################################### Transformations of data and feature engieering  ##################################################################################


############################ Transformations  ##########################

### LOG transformations to get a normal distribution of count. features
log_list = [col for col in dataset.columns if col not in ['ORIG_TIME', 'VEHICLE_ID', 'target', 'START_LOCATION', 'END_LOCATION', 'cycle_day', 'GeneralDescription', 'ManufacturerDescription', 'RegisterYear', 'VehicleModel', 'Level 07_Descripion', 'Day', 'DayOfweek', 'WeekOfYear', 'Month', 'Quarter', 'Year', 'Weekend','colFromIndex', 'target_20','target_30']]

for i in log_list:
    dataset[i] = dataset[i].apply(np.log)

#replacing -inf with 0
dataset = dataset.replace(-np.Inf, np.nan)
dataset = dataset.fillna(0)

    
#### one hot encoding of categorical non ranked features
dataset = pd.get_dummies(dataset, columns=['START_LOCATION'])
dataset = pd.get_dummies(dataset, columns=['END_LOCATION'])
dataset = pd.get_dummies(dataset, columns=['GeneralDescription'])
dataset = pd.get_dummies(dataset, columns=['ManufacturerDescription'])
dataset = pd.get_dummies(dataset, columns=['RegisterYear'])
dataset = pd.get_dummies(dataset, columns=['VehicleModel'])
dataset = pd.get_dummies(dataset, columns=['Level 07_Descripion'])


############################ Feature engineering  ##########################

#### Time feature engineering

# different time intervals 
dataset['Day'] = dataset['ORIG_TIME'].dt.day
dataset['DayOfweek'] = dataset['ORIG_TIME'].dt.dayofweek
dataset['WeekOfYear'] = dataset['ORIG_TIME'].dt.weekofyear
dataset['Month'] = dataset['ORIG_TIME'].dt.month
dataset['Quarter'] = dataset['ORIG_TIME'].dt.quarter
dataset['Year'] = dataset['ORIG_TIME'].dt.year

def weekend(row):
    if row['DayOfweek'] == 5:
        val = 1
    elif row['DayOfweek'] == 6:
        val = 1
    else:
        val = 0
    return val

dataset['Weekend'] = dataset.apply(weekend, axis=1) # looping over all rows (columns)


# we could one hot encode, but we do not have to if we want to capture the relationship betwwen different periods of time close to one another - 
dataset = pd.get_dummies(dataset, columns=['Day'])
dataset = pd.get_dummies(dataset, columns=['DayOfweek'])
dataset = pd.get_dummies(dataset, columns=['WeekOfYear'])
dataset = pd.get_dummies(dataset, columns=['Month'])
dataset = pd.get_dummies(dataset, columns=['Quarter'])
dataset = pd.get_dummies(dataset, columns=['Year'])

####### 1. Manual feature engineering

#dataset_man_features = dataset
dataset = dataset_man_features

dataset.columns = dataset.columns.astype(str)

Rolling_window = 7
features_window = ['SumMileage', 'SumIdleDuration', 'SumFuelUsed', 'MaxMechanicalGrade', 'SumDriveDuration', '40586']

for i in features_window:
    # 1. Lag of values --Z diff (1)
    dataset[i+'_lag'] = dataset.groupby(['VEHICLE_ID'])[i].shift(1)
    # 2. cummulative sum (count, total value) since start of cycle to event
    #new_window_features["wow_Cumsum"] = dataset['value'].cumsum()
    # 3. Week over week percentage change
    dataset[i+'_wow_percent_change'] = dataset.groupby(['VEHICLE_ID'])[i].pct_change(7)
    # 4. Week-over-week Difference
    dataset[i+'wow_difference'] = dataset.groupby(['VEHICLE_ID'])[i].diff(7)
    # 5. day-over-day Difference
    dataset[i+'_dod_difference'] = dataset.groupby(['VEHICLE_ID'])[i].diff(1)

##### 2. Manual sliding window aggregates
    # moving average of 7 windows (each row)
    dataset[i+'_rollingAverage'] = dataset.groupby(['VEHICLE_ID'],)[i].rolling(window=Rolling_window).mean().reset_index(level=0, drop=True)
    # 2. moving max of 7 windows (each row)
    dataset[i+'_rollingMax'] = dataset.groupby(['VEHICLE_ID'],)[i].rolling(window=Rolling_window).max().reset_index(level=0, drop=True)
    # 3. moving min of 7 windows (each row)
    dataset[i+'_rollingMin'] = dataset.groupby(['VEHICLE_ID'],)[i].rolling(window=Rolling_window).min().reset_index(level=0, drop=True)
    # 4. moving count of 7 windows (each row)
    #new_window_features[i+'_rollingCount'] = dataset.groupby(['VEHICLE_ID'],)[i].rolling(window=Rolling_window).count().reset_index(level=0, drop=True)
    # 5. moving standard deviataion of 7 windows (each row)
    dataset[i+'_rollingSD'] = dataset.groupby(['VEHICLE_ID'],)[i].rolling(window=Rolling_window).std().reset_index(level=0, drop=True)
 
## Filling na with 0
dataset = dataset.replace(-np.Inf, np.nan)
dataset = dataset.replace(np.Inf, np.nan)
dataset = dataset.fillna(0)



    


########### 3. automatic feature extraction using featuretools and TSFresh

#datasetfeatureengineering = dataset

dataset = datasetfeatureengineering

import featuretools as ft
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import os

dataset_featuretools = dataset

dataset_featuretools.columns = dataset_featuretools.columns.astype(str)

dataset_featuretools = dataset_featuretools.assign(ID = dataset_featuretools.VEHICLE_ID.astype(str) + '-' + dataset_featuretools.ORIG_TIME.astype(str))

# define label times which is the logic format we want as output (cutt_off time for each vehicle)
label_times = dataset_featuretools.loc[:, ['ID','ORIG_TIME','target']]


# drop values which are not needed (not user_id and time yet)
dataset_featuretools = dataset_featuretools.drop(['target'], axis = 1)


# Define entityset and
def make_entityset(data):
    es = ft.EntitySet('Dataset')
    es.entity_from_dataframe(dataframe=dataset_featuretools,
                             entity_id='recordings',
                             index='colFromIndex',
                             time_index='ORIG_TIME')

    es.normalize_entity(base_entity_id='recordings', 
                        new_entity_id='VEHICLE',
                        index='ID')

    es.normalize_entity(base_entity_id='recordings', 
                        new_entity_id='cycles',
                        index='cycle_day')
    return es
es = make_entityset(dataset_featuretools)
es

## Create new features with feature tools
from featuretools.primitives import make_agg_primitive
import featuretools.variable_types as vtypes
from featuretools.variable_types import DatetimeTimeIndex, Numeric

from tsfresh.feature_extraction.feature_calculators import (number_peaks, mean_abs_change, 
                                                            cid_ce, last_location_of_maximum, length)

## 1. Define custom aggregated primitives using TSFRESH

complexity = make_agg_primitive(lambda x: cid_ce(x, False),
                              input_types=[vtypes.Numeric],
                              return_type=vtypes.Numeric,
                              name="complexity")

def time_since_last(values, time=None):
    time_since = time - values.iloc[0]
    return time_since.total_seconds()

TimeSinceLast = make_agg_primitive(function=time_since_last,
                                   input_types=[DatetimeTimeIndex],
                                   return_type=Numeric,
                                   description="Time since last related instance",
                                   uses_calc_time=True)

##  2.1 build feature matrix for whole period

feature_matrix, features = ft.dfs(entityset=es, 
                      target_entity='VEHICLE',
                      agg_primitives=['last', 'sum', 'some', complexity, 'std', 'count', 'min', 'max' 'median', 'n_unique', TimesinceLast],
                      trans_primitives=[],
                      cutoff_time = label_times,
                      max_depth=1,
                      verbose=True,
                      n_jobs = -1)


##  2.2 build feature matrix for 30 days and 15 days (time-window features)

feature_defs = ft.dfs(entityset=es, 
                      target_entity='VEHICLE',
                      agg_primitives=['last', 'max', 'min', complexity],
                      trans_primitives=[],
                      max_depth=1,
                      verbose=True,
                      n_jobs = -1)

fm_30_days = ft.calculate_feature_matrix(entityset=es,
                                         features=feature_defs[1],
                                         cutoff_time=label_times,
                                         training_window="30 days")

fm_15_days = ft.calculate_feature_matrix(entityset=es,
                                         features=feature_defs[1],
                                         cutoff_time=label_times,
                                         training_window="15 days")

fm_30_days.merge(fm_15_days, left_index=True, right_index=True, suffixes=("__30_days", "__15_days"))


## combine features with dataframe

fm_encoded, features_encoded = ft.encode_features(feature_matrix,
                                                  features)

print("Number of features %s" % len(features_encoded))
fm_encoded.head(10)

dataset_feature_final = merge_features_labels(fm_encoded, label_times)

dataset_feature_final = dataset_feature_final.sort_values(by=['LAST(recordings.VEHICLE_ID)', 'ID'])

dataset_feature_final = dataset_feature_final.fillna(0)

# drop features that are dublicates
dataset_feature_final.drop(["ID", "ORIG_TIME", 'target'], axis=1, inplace=True)

# merge with current feature set
len(dataset) == len(dataset_feature_final)
dataset = pd.concat([dataset.reset_index(drop=True), dataset_feature_final], axis=1)


## dropping identifier, we do not need no more
dataset.drop(["colFromIndex"], axis=1, inplace=True)
dataset.drop(["ORIG_TIME"], axis=1, inplace=True)

#dataset_backup = dataset

dataset = dataset_backup



####### Feature extraction using PCA : Source https://chrisalbon.com/machine_learning/feature_engineering/feature_extraction_with_pca/

#dataset_pca = dataset 
dataset = dataset_pca 

# Import packages
import numpy as np
from sklearn import decomposition
from sklearn.preprocessing import StandardScaler

# View the shape of the dataset
dataset.shape

# Create a scaler object
sc = StandardScaler()

# Fit the scaler to the features and transform
dataset_std = sc.fit_transform(dataset)

# Create a pca object with the 20 components as a parameter
pca = decomposition.PCA(n_components=20,
                        random_state = 0)

# Fit the PCA and transform the data
Features_pca_std = pca.fit_transform(dataset_std)

# View the new feature data's shape
Features_pca_std.shape

# View the new feature data
Features_pca_std

## Add here hyper parameter optimization 

# explained varaince
pca.explained_variance_ratio_

pca.explained_variance_ratio_.sum()

## Merge with other dataset
Features_pca_std = pd.DataFrame(Features_pca_std)

len(dataset) == len(Features_pca_std)
dataset_including_pca = pd.concat([dataset.reset_index(drop=True), Features_pca_std], axis=1)

# dataset = dataset_including_pc


#----------------------------------------------------------- Transformations of data and feature engieering done  ------------------------------------------------------------


#################################################################### Partitioning of dataset  ##################################################################################

###### Partitioning of dataset

### defining independent and dependent features before Feature engineerin and sampling
x = np.array(dataset.iloc[:, dataset.columns != 'target'])
y = np.array(dataset.iloc[:, dataset.columns == 'target'])
print('Shape of x: {}'.format(x.shape))
print('Shape of y: {}'.format(y.shape))
#x = pd.DataFrame(x)
#y = pd.DataFrame(y)


###### time-dependent split - NO SHUFFLE = vehcile dependent split

from sklearn.model_selection import train_test_split

Last_vehichle_index = dataset.reset_index()
Last_vehichle_index1 = Last_vehichle_index['VEHICLE_ID'].diff()[Last_vehichle_index['VEHICLE_ID'].diff() != 0].index.values

int_index = round(len(Last_vehichle_index1) * 0.70)

len_dataset = len(dataset)
splitting = 1-((Last_vehichle_index1[int_index])/len_dataset)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=splitting, shuffle=False, random_state=0)

print("Number transactions X_train dataset: ", x_train.shape) # change layout
print("Number transactions y_train dataset: ", y_train.shape) # change layout
print("counts of labels training '1': {}".format(sum(y_train==1))) # change layout
print(" counts of labels training '0': {} \n".format(sum(y_train==0))) # change layout

print("Number transactions X_test dataset: ", x_test.shape) # change layout
print("Number transactions y_test dataset: ", y_test.shape) # change layout
print("counts of labels test '1': {}".format(sum(y_test==1))) # change layout
print(" counts of labels test '0': {} \n".format(sum(y_test==0))) # change layout








#----------------------------------------------------------- Partitioning of dataset done  ------------------------------------------------------------


#################################################################### Feature scalling ##################################################################################

## Feature scaling - Standardization 
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
x_train = sc_x.fit_transform(x_train)
x_test = sc_x.transform(x_test) # we do not have to fit, as it is already fitted to the training set.

#----------------------------------------------------------- Feature scalling Done  ------------------------------------------------------------


###############################################  Sampling techniques - Adjusting dataset for Unballanced sample (Target variable distribution) ################################
# source: # https://elitedatascience.com/imbalanced-classes

# 1. Option - SMOTE ALGORITHM
from imblearn.over_sampling import SMOTE

sm = SMOTE(k_neighbors=3, random_state=0)
x_train_res, y_train_res = sm.fit_sample(x_train, y_train.ravel()) 
#x_train_res = pd.DataFrame(x_train_res)
#y_train_res = pd.DataFrame(y_train_res)

print('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape)) # change layout
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape)) # change layout

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1))) # change layout
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0))) # change layout


#--------------------------------------------------------- Sampling techniques - Adjusting dataset for Unballanced sample (Target variable distribution) Done  ------------------------------------------------------------



#################################################################### Dimensionality reduction  ##################################################################################



### 1. PCA for dimensionality reduction
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA, KernelPCA

# Create a PCA that will retain 99% of the variance
pca_reduction = PCA(n_components=0.99, whiten=True)


# Conduct PCA
# fit only on training set, just transform on testing data
X_pca_reduction_train = pca_reduction.fit_transform(x_train) # transforming the data into the principle components
X_pca_reduction_test = pca_reduction.transform(x_test)

# Show results
print('Original number of features:', x_train.shape[1])
print('Reduced number of features:', X_pca_reduction_train.shape[1])



### 2. Truncated SVD for dimensions reduction ###- Takes around 15-20 min to run

# Selecting the Best number of Components for TSVD
# Load libraries
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import TruncatedSVD
from scipy.sparse import csr_matrix
import numpy as np

# Load the data

# Make sparse matrix
X_sparse_train = csr_matrix(x_train)

X_sparse_test = csr_matrix(x_test)


# Create and run an TSVD with one less than number of features
tsvd = TruncatedSVD(n_components=X_sparse_train.shape[1]-1)

X_tsvd = tsvd.fit(x_train, y_train)

# List of explained variances
tsvd_var_ratios = tsvd.explained_variance_ratio_

# Create a function
def select_n_components(var_ratio, goal_var: float) -> int:
    # Set initial variance explained so far
    total_variance = 0.0
    
    # Set initial number of features
    n_components = 0
    
    # For the explained variance of each feature:
    for explained_variance in var_ratio:
        
        # Add the explained variance to the total
        total_variance += explained_variance
        
        # Add one to the number of components
        n_components += 1
        
        # If we reach our goal level of explained variance
        if total_variance >= goal_var:
            # End the loop
            break
            
    # Return the number of components
    return n_components

# Run function
components = select_n_components(tsvd_var_ratios, 0.99)

## Run TSVD (Truncated SVD)

# Create a TSVD
tsvd_run = TruncatedSVD(n_components=components)

# Conduct TSVD on sparse matrix and tranforming data into TSVD components
X_tsvd_train = tsvd_run.fit(X_sparse_train).transform(X_sparse_train)

X_tsvd_test = tsvd_run.transform(X_sparse_test)

# Show results
print('Original number of features:', X_sparse_train.shape[1])
print('Reduced number of features:', X_tsvd_train.shape[1])

# Sum of first three components' explained variance ratios
tsvd_run.explained_variance_ratio_.sum()




#----------------------------------------------------------- Dimensionality reduction Done  ------------------------------------------------------------
####################################################### Choosing method for next steps #########################################################################################


x_train_default = x_train
x_test_default = x_test
 
y_test_default = y_test
y_train_default = y_train


### Fitting
x_train_14 = x_train
x_test_14 = x_test
 
y_test_14 = y_test
y_train_14 = y_train

x_train_20 = x_train
x_test_20 = x_test
 
y_test_20 = y_test
y_train_20 = y_train

x_train_30 = x_train
x_test_30 = x_test
 
y_test_30 = y_test
y_train_30 = y_train

x_train= x_train_14
x_test = x_test_14
 
y_test= y_test_14
y_train = y_train_14

x_train= x_train_20
x_test = x_test_20
 
y_test= y_test_20
y_train = y_train_20



# default
x_train = x_train_default
x_test = x_test_default

y_train = y_train_default


# SMOTE oversampling
x_train = x_train_res
y_train = y_train_res



# PCA - Dimension reduction
x_train = X_pca_reduction_train
x_test = X_pca_reduction_test

Pca_column_names = pd.DataFrame(X_pca_reduction_train)
Pca_column_names.columns = ['COM1', 'COM2', 'COM3', 'COM4', 'COM5', 'COM6', 'COM7', 'COM8', 'COM9', 'COM10', 'COM11', 'COM12', 'COM13', 'COM14', 'COM15', 'COM16', 'COM17', 'COM18', 'COM19', 'COM20']
### automate this with a arange function. 

# PCA - TSVD reduction
x_train = X_tsvd_train
x_test = X_tsvd_test

tsvd_column_names = pd.DataFrame(X_tsvd_train)
tsvd_length =  X_tsvd_train.shape[1]

tsvd_list = [int(x) for x in np.linspace(0, tsvd_length-1, num = tsvd_length)]

for i in tsvd_list:
    tsvd_list[i]= str(tsvd_list[i])

tsvd_column_names.columns = tsvd_list






#----------------------------------------------------------- Choosing method for next steps  ------------------------------------------------------------



####################################################### Feature set selection / Evaluation #########################################################################################
# Source: https://stat.ethz.ch/~nicolai/stability.pdf  

from joblib import Memory
from joblib import Parallel
from joblib import delayed

# Store the column/feature names into a list "colnames"

## Default & SMOTE
#datasetdrop = datasetdrop.drop(['VEHICLE_ID'], axis=1)
#colnames = datasetdrop.columns[datasetdrop.columns != 'target']
colnames = dataset.columns[dataset.columns != 'target']


## PCA
#colnames = Pca_column_names.columns[Pca_column_names.columns != 'target']

## TSVD
#colnames = tsvd_column_names.columns[tsvd_column_names.columns != 'target']




# Define dictionary to store our rankings
ranks = {}
# Create our function which stores the feature rankings to the ranks dictionary
from sklearn.preprocessing import MinMaxScaler
def ranking(ranks, names, order=1):
    minmax = MinMaxScaler()
    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]
    ranks = map(lambda x: round(x,2), ranks)
    return dict(zip(names, ranks))




#### 1. Stability selection via Randomized Lasso method
# the Stability Selection method is conveniently inbuilt into sklearn's randomized lasso mode
from sklearn.linear_model import (LinearRegression, RidgeClassifier, Lasso, RandomizedLasso)
rlasso = RandomizedLasso(alpha=0.04)
rlasso.fit(x_train, y_train.ravel())
ranks["rlasso/Stability"] = ranking(np.abs(rlasso.scores_), colnames)
print('finished')



##### 2. Recursive Feature Elimination (RFE) - Logistic regression example
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

model = LogisticRegression()
# model = LinearRegression(normalize=True) # for regression problem
rfe = RFE(model, n_features_to_select = 1, verbose = 3) # choose number of features
fit = rfe.fit(x_train, y_train)
ranks["RFE_log"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)
print("Num Features: %d") #% fit.n_features_
print("Selected Features: %s") #% fit.support_
print("Feature Ranking: %s") #% fit.ranking_

print(fit.n_features_) # number of features choosen
print(fit.support_) # True if working well
print(fit.ranking_) # needs to be 1 or as close to 1 as possible

##### 3. Using Linear model feature ranking Lasso
lasso = Lasso(alpha=.05)
lasso.fit(x_train, y_train)
ranks["Lasso"] = ranking(np.abs(lasso.coef_), colnames);

##### 4. Feature Importance evaluation (Random Forest)
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_jobs=-1, n_estimators=100, verbose=3)
rf.fit(x_train, y_train.ravel())
ranks["RF"] = ranking(rf.feature_importances_, colnames);

##### 5. Feature Importance evaluation (Extra trees)
from sklearn.ensemble import ExtraTreesClassifier # could also use Random forests or Extra Trees
model = ExtraTreesClassifier(n_jobs=-1)
model.fit(x_train, y_train.ravel())
ranks["ETC"] = ranking(model.feature_importances_, colnames); 

##### 6. feature importance evaluation (xgboost)
from xgboost import XGBClassifier
modelXGB = XGBClassifier()
modelXGB.fit(x_train, y_train.ravel())
ranks["XGB"] = ranking(modelXGB.feature_importances_, colnames); 


##### Building the Feature Ranking Matrix
# Create empty dictionary to store the mean value calculated from all the scores
import seaborn as sns
r = {}
for name in colnames:
    r[name] = round(np.mean([ranks[method][name] 
                             for method in ranks.keys()]), 2)
 
methods = sorted(ranks.keys())
ranks["Mean"] = r
methods.append("Mean")
 
print("\t%s" % "\t".join(methods))
for name in colnames:
    print("%s\t%s" % (name, "\t".join(map(str, 
                         [ranks[method][name] for method in methods]))))
    
# Put the mean scores into a Pandas dataframe
meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])

# Sort the dataframe
meanplot = meanplot.sort_values('Mean Ranking', ascending=False)

# Let's plot the 50 most important features
sns.set(font_scale=1.7)
sns.set_style("whitegrid")
sns.factorplot(x="Mean Ranking", y="Feature", data = meanplot.head(50), kind="bar", 
               size=20, aspect=1.9, palette='coolwarm')


#----------------------------------------------------------- Feature set selection / Evaluation Done  ------------------------------------------------------------


################################################################################ Classification with different classifiers - Cost-sensitive training - Weigted by sample distribution ################


#### Linear models 

# Logistic regression
from sklearn.linear_model import LogisticRegression
classifier_log = LogisticRegression(class_weight = 'balanced', # penalize
                                    random_state = 0,
                                    n_jobs = -1)
classifier_log.fit(x_train, y_train.ravel())


# Logistic regression with penalty
from sklearn.linear_model import LogisticRegression
classifier_log_pen = LogisticRegression(class_weight = 'balanced', # penalize
                                    penalty = 'l1',
                                    random_state = 0,
                                    n_jobs = -1)
classifier_log_pen.fit(x_train, y_train.ravel())


# Penealized SVM
#https://elitedatascience.com/imbalanced-classes -- See point 4. 
#from sklearn.svm import SVC
#from sklearn.ensemble import BaggingClassifier

#n_estimators_SVC = 7 # Number of vehicles
#classifier_SVC_lin_B = BaggingClassifier(SVC(kernel='linear',
#                                             class_weight='balanced',
#                                             probability=True,
#                                             random_state = 0),
#                                                             max_samples=1.0 / n_estimators_SVC,
#                                                             n_estimators=n_estimators_SVC,
#                                                             n_jobs = -1)

#classifier_SVC_lin_B.fit(x_train, y_train.ravel())


#K-nearest Neighbors (K-NN) (non-linear but not highly complex)
from sklearn.neighbors import KNeighborsClassifier
classifier_KNN =  KNeighborsClassifier(n_neighbors = 5,
                                   metric = 'minkowski',
                                   p = 2)
classifier_KNN.fit(x_train, y_train.ravel())

##### non-linear models

# Fitting Kernel (non-linear) Suppoer vector machine
#n_estimators_SVC = 7 # baggings = Number of vehicles
#classifier_SVC = BaggingClassifier(SVC(kernel='rbf',
#                                             class_weight='balanced',
#                                             probability=True,
#                                             random_state = 0),
#                                                             max_samples=1.0 / n_estimators_SVC,
#                                                             n_estimators=n_estimators_SVC,
#                                                             n_jobs = -1)

#classifier_SVC.fit(x_train, y_train.ravel())

# Naive Bayes
from sklearn.naive_bayes import GaussianNB
classifier_NB = GaussianNB() # not parameters to include
classifier_NB.fit(x_train, y_train.ravel())


# Use decision tree based algorithms - they work well with unballanced samples (before adjusting balances)
# we do not have to do feature scalling! and other pre-processing steps
from sklearn.ensemble import RandomForestClassifier
classifier_Forrest = RandomForestClassifier(n_estimators = 100, 
                                    random_state = 0,
                                    n_jobs = -1,
                                    verbose = 1,
                                    class_weight = "balanced")
classifier_Forrest.fit(x_train, y_train.ravel())

## XGBoost
#x_train_XGB = x_train_res.as_matrix()

from xgboost import XGBClassifier
  
# unbalanced sample adjustments
dataset_majority = dataset[dataset['target']==0]
dataset_minority = dataset[dataset['target']==1]
pos_weight = len(dataset_majority) / len(dataset_minority)

classifier_XGB = XGBClassifier(learning_rate = 0.2,
                           scale_pos_weight = pos_weight, 
                           max_depth = 6,
                           n_estimators = 100,
                          objective = "binary:logistic",
                          eval_metric = "auc", # as we do not have any F1
                          random_state = 0,
                          n_jobs = -1)
classifier_XGB.fit(x_train, y_train.ravel())


## adaboost
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Create adaboost-decision tree classifer object
classifier_ADA = AdaBoostClassifier(DecisionTreeClassifier(class_weight='balanced',
                                                           max_depth=6),
                                    n_estimators=100,
                                    learning_rate=1,
                                    random_state=0)

classifier_ADA.fit(x_train, y_train.ravel())


## logiboost classifier

from logitboost import LogitBoost
from sklearn.tree import DecisionTreeRegressor
classifier_loB = LogitBoost(DecisionTreeRegressor(max_depth=6),
                            n_estimators=100,
                            learning_rate=1,
                            random_state=0,
                            weight_trim_quantile=0.05)
classifier_loB.fit(x_train, y_train.ravel())



# Predicting the Test set results for one model
y_pred = classifier_Forrest.predict(x_test) # class 
y_pred_prob= classifier_Forrest.predict_proba(x_test)

y_pred = i.predict(x_test) # class 
y_pred_prob= i.predict_proba(x_test)
# Making the Confusion Matrix

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)




#----------------------------------------------------------- Classification with different classifiers - Cost-sensitive training - Weigted by sample distribution Done  ------------------------------------------------------------







################################################################################ Model evaluation step 1 ################

from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report

####### Evaluation matrix

First_col = pd.Series()
list_eva = [classifier_log, classifier_log_pen, classifier_KNN, classifier_NB, classifier_Forrest, classifier_XGB, classifier_ADA, classifier_loB]
df_eva = pd.Series(First_col, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity', 'Specificity', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])

for i in list_eva:
    y_pred = i.predict(x_test) # class 
    y_pred_prob= i.predict_proba(x_test)

    # Making the Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    TP = cm[1, 1]
    TN = cm[0, 0]

    FP = cm[0,1]
    FN = cm[1,0]

    tot_obs = TP + TN + FP + FN
    tot_cor = TP + TN 
    tot_fal = FP + FN

    P = np.count_nonzero(y_test == 1) # real positive cases/events of all observations
    N =  np.count_nonzero(y_test == 0) # real negative cases/events of all observations

    ## Calculate evaluation measures
    Acc_rate = tot_cor / tot_obs # print(accuracy_score(y_test, y_pred))
    Acc_rate_balanced = balanced_accuracy_score(y_test, y_pred)
    Sensitivity = TP / P # True possitive rate / Sensitivity / recall
    Specificity = TN / N # True negative rate / Specificity
    Precision = TP / (TP + FP) # positive prediction value
    Precision_score  = precision_score(y_test, y_pred)
    Miss_rate_False_negative_rate = (FN / (FN + TP)) # False negative rate
    fall_out_False_positive_rate = FP / N # false positive rate
    F1 = (2*TP)/(2*TP+FP+FN)
    F1_binary = f1_score(y_test, y_pred, average='binary') # harmonic mean of precision and sensitivity
    y_pred_prob = [p[1] for p in y_pred_prob]
    Auc = roc_auc_score(y_test, y_pred_prob)
    
    scores = np.append(Acc_rate, Acc_rate_balanced)
    scores = np.append(scores, Sensitivity)
    scores = np.append(scores, Specificity)
    scores = np.append(scores, Precision_score)
    scores = np.append(scores, Miss_rate_False_negative_rate)
    scores = np.append(scores, fall_out_False_positive_rate)
    scores = np.append(scores, F1)
    scores = np.append(scores, F1_binary)
    scores = np.append(scores, Auc)

    df_i = pd.Series(scores, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity', 'Specificity', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])
    df_eva = pd.concat([df_eva, df_i], axis=1)
    
###
df_eva = df_eva.iloc[:,1:]
df_eva.columns = ['classifier_log', 'classifier_log_pen', 'classifier_KNN', 'classifier_NB', 'classifier_Forrest', 'classifier_XGB', 'classifier_ADA', 'classifier_loB']



#-----------------------------------------------------------------------------  Model evaluation step 1 done  ------------------------------------------------------------

################################################################################ Model evaluation step 2 - Random Grid search for hyperparameter opotimization ################


# Random Grid search for hyperparameter optimization

# ###################### 1. random forest hyperparameters
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import RandomizedSearchCV



classifier_rf = RandomForestClassifier(random_state = 0, class_weight = 'balanced')

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt', 'log2']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 5)]
# Minimum number of samples required at each leaf node
min_samples_leaf = [int(x) for x in np.linspace(start = 1, stop = 10, num = 5)]
# Method of selecting samples for training each tree
bootstrap = [False]
#class weight
class_weight = ['balanced']

# Create the random grid
random_grid_rf = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap,
               'class_weight': class_weight}

# Random search of parameters, using 1 fold cross validation, for 200 different iterations 
classifier_rf_random = RandomizedSearchCV(estimator = classifier_rf, param_distributions = random_grid_rf, n_iter = 200,  cv = ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['recall' ,'roc_auc', 'precision'], refit = 'precision', verbose=3, random_state=0, n_jobs = -1)
# Fit the random search model
classifier_rf_random.fit(x_train, y_train.ravel())

# best performing parameters
classifier_rf_random.best_params_
classifier_rf_random.best_score_ 

# evaluation
y_pred = classifier_rf_random.predict(x_test) # class 
y_pred_prob= classifier_rf_random.predict_proba(x_test)

# Making the Confusion Matrix

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)



######################### 2. XGBoosting hyperparameters
from xgboost import XGBClassifier
  
# unbalanced sample adjustments
dataset_majority = dataset[dataset['target']==0] # change this value to default 
dataset_minority = dataset[dataset['target']==1] # change this value to default
pos_weight = len(dataset_majority) / len(dataset_minority)

classifier_XGB = XGBClassifier(learning_rate = 0.2,
                           scale_pos_weight = pos_weight, 
                           objective = "binary:logistic",
                          random_state = 0)

learning_rate_XGB = [0.1, 0.2] 
n_estimators_XGB = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]
max_depth_XGB = [int(x) for x in np.linspace(10, 110, num = 11)]
min_child_weight_XGB = [1, 3, 5]
gamma_XGB = [round(x,2) for x in np.linspace(0.2, 1, num = 5)]
colsample_bytree_XGB = [round(x,2) for x in np.linspace(0.1, 1, num = 4)]
reg_lambda_XGB = [round(x,2) for x in np.linspace(0, 1, num = 5)]
booster = ['gbtree', 'dart']

random_grid_XGB = {'learning_rate': learning_rate_XGB,
               'n_estimators': n_estimators_XGB,
               'max_depth': max_depth_XGB,
               'min_child_weight': min_child_weight_XGB,
               'gamma': gamma_XGB,
               'colsample_bytree': colsample_bytree_XGB,
               'reg_lambda': reg_lambda_XGB,
               'booster': booster
               }

# Set random seed
np.random.seed(0)

# Random search of parameters, using 1 fold cross validation, for 200 different iterations 
classifier_XGB_random = RandomizedSearchCV(estimator = classifier_XGB, param_distributions = random_grid_XGB, n_iter = 100,  cv = ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['f1' ,'roc_auc', 'precision'], refit = 'precision', verbose=3, random_state=0, n_jobs = -1)
# Fit the random search model
classifier_XGB_random.fit(x_train, y_train.ravel())

# best performing parameters
classifier_XGB_random.best_params_
classifier_XGB_random.best_score_ 

# Evaluation
y_pred = classifier_XGB_random.predict(x_test) # class 
y_pred_prob= classifier_XGB_random.predict_proba(x_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)


# ###################### 3. Adaboost hyperparameters
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

DTC = DecisionTreeClassifier(random_state = 0, max_features = "auto", class_weight = "balanced", max_depth = 6)

classifier_ada = AdaBoostClassifier(base_estimator = DTC)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 3000, num = 30)]

learning_rate = [float(x) for x in np.linspace(start = 0.01, stop = 1, num = 10)]


# Create the random grid
random_grid_ada = {'n_estimators': n_estimators,
               'learning_rate': learning_rate}

# Random search of parameters, using 1 fold cross validation, for 200 different iterations 
classifier_ada_random = RandomizedSearchCV(estimator = classifier_ada, param_distributions = random_grid_ada, n_iter = 200,  cv = ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['f1' ,'roc_auc', 'precision'], refit = 'precision', verbose=3, random_state=0, n_jobs = -1)
# Fit the random search model
classifier_ada_random.fit(x_train, y_train.ravel())

# best performing parameters
classifier_ada_random.best_params_
classifier_ada_random.best_score_ 

# Evaluation
y_pred = classifier_ada_random.predict(x_test) # class 
y_pred_prob= classifier_ada_random.predict_proba(x_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)


# ###################### 4. LogiBoost hyperparameters
from logitboost import LogitBoost
from sklearn.tree import DecisionTreeRegressor

DTR = DecisionTreeRegressor(random_state = 0, max_features = "auto", max_depth = 6)

classifier_logi = LogitBoost(base_estimator = DTR)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 3000, num = 30)]
learning_rate = [float(x) for x in np.linspace(start = 0.01, stop = 1, num = 10)]
weight_trim_quantile = [float(x) for x in np.linspace(start = 0.001, stop = 0.08, num = 6)]



# Create the random grid
random_grid_logi = {'n_estimators': n_estimators,
               'learning_rate': learning_rate,
               'weight_trim_quantile': weight_trim_quantile}

# Random search of parameters, using 1 fold cross validation, for 200 different iterations 
classifier_logi_random = RandomizedSearchCV(estimator = classifier_logi, param_distributions = random_grid_logi, n_iter = 100,  cv = ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['f1' ,'roc_auc', 'precision'], refit = 'precision', verbose=3, random_state=0, n_jobs = -1)
# Fit the random search model
classifier_logi_random.fit(x_train, y_train.ravel())

# best performing parameters
classifier_logi_random.best_params_
classifier_logi_random.best_score_ 

######################### 5. KNN hyperparameters
from sklearn.neighbors import KNeighborsClassifier
  
classifier_KNN =  KNeighborsClassifier(algorithm = 'auto')


n_neighbors = [int(x) for x in np.linspace(start = 2, stop = 30, num = 20)]
p = [1, 2]
weights = ['uniform', 'distance']

random_grid_KNN = {'n_neighbors': n_neighbors,
               'weights': weights,
               'p': p}
               

# Set random seed
np.random.seed(0)

# Random search of parameters, using 1 fold cross validation, for 200 different iterations 
classifier_KNN_random = RandomizedSearchCV(estimator = classifier_KNN, param_distributions = random_grid_KNN, n_iter = 200,  cv = ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['f1' ,'roc_auc', 'precision'], refit = 'precision', verbose=3, random_state=0, n_jobs = -1)
# Fit the random search model
classifier_KNN_random.fit(x_train, y_train.ravel())

# best performing parameters
classifier_KNN_random.best_params_
classifier_KNN_random.best_score_ 

# Evaluation
y_pred = classifier_KNN_random.predict(x_test) # class 
y_pred_prob= classifier_KNN_random.predict_proba(x_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)



##################### models evaluation

First_col = pd.Series()
list_eva_Rgrid = [classifier_rf_random, classifier_XGB_random, classifier_ada_random, classifier_KNN_random]
df_eva_Rgrid = pd.Series(First_col, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity', 'Specificity', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])

for i in list_eva_Rgrid:
    y_pred = i.predict(x_test) # class 
    y_pred_prob= i.predict_proba(x_test)

    # Making the Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    TP = cm[1, 1]
    TN = cm[0, 0]

    FP = cm[0,1]
    FN = cm[1,0]

    tot_obs = TP + TN + FP + FN
    tot_cor = TP + TN 
    tot_fal = FP + FN

    P = np.count_nonzero(y_test == 1) # real positive cases/events of all observations
    N =  np.count_nonzero(y_test == 0) # real negative cases/events of all observations

    ## Calculate evaluation measures
    Acc_rate = tot_cor / tot_obs # print(accuracy_score(y_test, y_pred))
    Acc_rate_balanced = balanced_accuracy_score(y_test, y_pred)
    Sensitivity = TP / (TP+FN) # True possitive rate / Sensitivity / recall
    Specificity = TN / (TN+FP) # True negative rate / Specificity
    Precision = TP / (TP + FP) # positive prediction value
    Precision_score  = precision_score(y_test, y_pred)
    Miss_rate_False_negative_rate = (FN / (FN + TP)) # False negative rate
    fall_out_False_positive_rate = FP / N # false positive rate
    F1 = (2*TP)/(2*TP+FP+FN)
    F1_binary = f1_score(y_test, y_pred, average='binary') # harmonic mean of precision and sensitivity
    y_pred_prob = [p[1] for p in y_pred_prob]
    Auc = roc_auc_score(y_test, y_pred_prob)
    
    scores = np.append(Acc_rate, Acc_rate_balanced)
    scores = np.append(scores, Sensitivity)
    scores = np.append(scores, Specificity)
    scores = np.append(scores, Precision_score)
    scores = np.append(scores, Miss_rate_False_negative_rate)
    scores = np.append(scores, fall_out_False_positive_rate)
    scores = np.append(scores, F1)
    scores = np.append(scores, F1_binary)
    scores = np.append(scores, Auc)

    df_i = pd.Series(scores, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity', 'Specificity', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])
    df_eva_Rgrid = pd.concat([df_eva_Rgrid, df_i], axis=1)
    
###
df_eva_Rgrid = df_eva_Rgrid.iloc[:,1:]
df_eva_Rgrid.columns = ['classifier_rf_random', 'classifier_XGB_random', 'classifier_ada_random', 'classifier_KNN_random']


#-----------------------------------------------------------------------------  Model evaluation step 2 - Random Grid search for hyperparameter opotimization #------------------------------------------------------------------



################################################################################ Model evaluation step 3 - Grid search for hyperparameter optimization ############################################################################
# Load libraries
import numpy as np
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import ShuffleSplit
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report

# Set random seed
np.random.seed(0)

# Create a pipeline
pipe = Pipeline([('classifier', RandomForestClassifier())])

# Create space of candidate learning algorithms and their hyperparameters
search_space = [{'classifier': [RandomForestClassifier()], 'classifier__n_estimators': [300, 350, 400, 450, 500], 'classifier__max_depth': [8, 10, 12], 'classifier__max_features': ['auto'], 'classifier__min_samples_leaf': [1, 2, 3], 'classifier__min_samples_split': [7, 8, 9], 'classifier__class_weight': ['balanced']},
                {'classifier': [XGBClassifier()], 'classifier__scale_pos_weight': [pos_weight], 'classifier__objective':['binary:logistic'], 'classifier__n_estimators': [80 ,90, 100, 110], 'classifier__max_depth': [70, 80, 90], 'classifier__min_child_weight': [3, 4, 5], 'classifier__gamma': [0.4, 0.5, 0.6], 'classifier__colsample_bytree': [0.3, 0.4, 0.5], 'classifier__reg_lambda': [0.9, 1]}
                ]


# Create grid search 
clf = GridSearchCV(estimator = pipe, param_grid = search_space, cv=ShuffleSplit(test_size=splitting, n_splits=1), scoring = ['precision', 'f1' ,'roc_auc'], refit = 'precision', verbose=3, n_jobs = -1)
# inklude multiple scoring measures

# Fit grid search
best_model = clf.fit(x_train, y_train.ravel()) # we should adjust the splitting. 

best_model.get_params().keys()

# View best model
best_model_predict = best_model.best_estimator_.get_params()['classifier'] 
best_model.best_score_ 
best_model.best_params_
best_model.scorer_ 


results = best_model.cv_results_ 
y_pred = best_model_predict.predict(x_test) # class 
y_pred_prob= best_model_predict.predict_proba(x_test)

# Making the Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Evaluation model

First_col = pd.Series()
list_eva_grid = [best_model_predict]
df_eva_grid = pd.Series(First_col, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity_TPR', 'Specificity_TNR', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])

for i in list_eva_grid:
    y_pred = i.predict(x_test) # class 
    y_pred_prob= i.predict_proba(x_test)

    # Making the Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)

    TP = cm[1, 1]
    TN = cm[0, 0]

    FP = cm[0,1]
    FN = cm[1,0]

    tot_obs = TP + TN + FP + FN
    tot_cor = TP + TN 
    tot_fal = FP + FN

    P = np.count_nonzero(y_test == 1) # real positive cases/events of all observations
    N =  np.count_nonzero(y_test == 0) # real negative cases/events of all observations

    ## Calculate evaluation measures
    Acc_rate = tot_cor / tot_obs # print(accuracy_score(y_test, y_pred))
    Acc_rate_balanced = balanced_accuracy_score(y_test, y_pred)
    Sensitivity_TPR = TP / (TP+FN) # True possitive rate / Sensitivity / recall
    Specificity_TNR = TN / (TN+FP) # True negative rate / Specificity
    Precision = TP / (TP + FP) # positive prediction value
    Precision_score  = precision_score(y_test, y_pred)
    Miss_rate_False_negative_rate = (FN / (FN + TP)) # False negative rate
    fall_out_False_positive_rate = FP / N # false positive rate
    F1 = (2*TP)/(2*TP+FP+FN)
    F1_binary = f1_score(y_test, y_pred, average='binary') # harmonic mean of precision and sensitivity
    y_pred_prob = [p[1] for p in y_pred_prob]
    Auc = roc_auc_score(y_test, y_pred_prob)
    
    scores = np.append(Acc_rate, Acc_rate_balanced)
    scores = np.append(scores, Sensitivity_TPR)
    scores = np.append(scores, Specificity_TNR)
    scores = np.append(scores, Precision_score)
    scores = np.append(scores, Miss_rate_False_negative_rate)
    scores = np.append(scores, fall_out_False_positive_rate)
    scores = np.append(scores, F1)
    scores = np.append(scores, F1_binary)
    scores = np.append(scores, Auc)

    df_i = pd.Series(scores, index=['Acc_rate', 'Acc_rate_balanced', 'Sensitivity_TPR', 'Specificity_TNR', 'Precision_score', 'Miss_rate/False_negative_rate', 'fall_out/False_positive_rate', 'F1', 'F1_binary', 'Auc'])
    df_eva_grid = pd.concat([df_eva_grid, df_i], axis=1)
    
###
df_eva_grid = df_eva_grid.iloc[:,1:]
df_eva_grid.columns = ['best_model_predict']






###### Evaluate 14, 20, 30 days
#Taking one at the time in this case 14 (15) days are used

dataset_majority = dataset[dataset['target']==0]
dataset_minority = dataset[dataset['target']==1]
pos_weight = len(dataset_majority) / len(dataset_minority)

classifier_XGB_final_14 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=0.3, gamma=0.5, learning_rate=0.1,
       max_delta_step=0, max_depth=70, min_child_weight=3, missing=None,
       n_estimators=90, n_jobs=1, nthread=None,
       objective='binary:logistic', random_state=0, reg_alpha=0,
       reg_lambda=1, scale_pos_weight=3.4968895800933124, seed=None,
       silent=True, subsample=1)

classifier_XGB_final_14.fit(x_train, y_train.ravel()) # 3.420503909643788
classifier_XGB_final_20.fit(x_train, y_train.ravel()) # 2.465505226480836
classifier_XGB_final_30.fit(x_train, y_train.ravel())


y_pred = classifier_XGB_final_14.predict(x_test) # class 
y_pred_prob= classifier_XGB_final_14.predict_proba(x_test)

## Confusion matrix
import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=False, figsize = (15,10), text_fontsize = 'large', title_fontsize  = 'large')




## Feature importance of best performing model
# best model feature importance
colnames = dataset.columns[dataset.columns != 'target']

# Define dictionary to store our rankings
ranks = {}
# Create our function which stores the feature rankings to the ranks dictionary
from sklearn.preprocessing import MinMaxScaler
def ranking(ranks, names, order=1):
    minmax = MinMaxScaler()
    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]
    ranks = map(lambda x: round(x,2), ranks)
    return dict(zip(names, ranks))

ranks["XGB"] = ranking(best_model_predict.feature_importances_, colnames); 


# visualization
import seaborn as sns
r = {}
for name in colnames:
    r[name] = round(np.mean([ranks[method][name] 
                             for method in ranks.keys()]), 2)
 
methods = sorted(ranks.keys())
ranks["Mean"] = r
methods.append("Mean")
 
print("\t%s" % "\t".join(methods))
for name in colnames:
    print("%s\t%s" % (name, "\t".join(map(str, 
                         [ranks[method][name] for method in methods]))))
    
# Put the mean scores into a Pandas dataframe
meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])

# Sort the dataframe
meanplot = meanplot.sort_values('Mean Ranking', ascending=False)
meanplot = meanplot.iloc[1:, :] 

# Let's plot the 50 most important features
sns.set(font_scale=3)
sns.set_style("white")
sns.factorplot(x="Mean Ranking", y="Feature", data = meanplot.head(20), kind="bar", 
               size=20, aspect=1, palette='coolwarm')

#-----------------------------------------------------------------------------  Model evaluation step 3 - Model evaluation step 3 - Grid search for hyperparameter optimization #------------------------------------------------------------------

################################################################################ Evaluation visualization ############################################################################

## Confusion matrix
import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=False, figsize = (15,10), text_fontsize = 'large', title_fontsize  = 'large')


###### 1.1 RUC curve
import scikitplot as skplt
import matplotlib.pyplot as plt

skplt.metrics.plot_roc_curve(y_test, y_pred_prob, figsize = (15,10), text_fontsize = 'large', title_fontsize  = 'large')



# 2. Precision-recall curve
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
from sklearn.utils.fixes import signature

precision, recall, _ = precision_recall_curve(y_test, y_pred)

# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument
step_kwargs = ({'step': 'post'}
               if 'step' in signature(plt.fill_between).parameters
               else {})
plt.step(recall, precision, color='b', alpha=0.2,
         where='post')
plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(
          average_precision))


### 3 plotting learning curves for the training set only
from sklearn.model_selection import train_test_split

x1 = pd.DataFrame(x)
Last_vehichle_index2 = x1.reset_index()
Last_vehichle_index3 = Last_vehichle_index2[0].diff()[Last_vehichle_index2[0].diff() != 0].index.values

int_index = round(len(Last_vehichle_index3) * 0.75)

splitting = 1-((Last_vehichle_index1[int_index])/len_dataset)

train_sizess = [100, 500, 1000, 2000, 3000, 3920] # because we have many splits we cannot do it probably.
# for illustrative purposes, we do it anyway.

# Create CV training and test scores for various training set sizes
train_sizes, train_scores, validation_scores = learning_curve(classifier_XGB_final_14, 
                                                        x, 
                                                        y,
                                                        # Number of folds in cross-validation
                                                        cv=ShuffleSplit(test_size=splitting, n_splits=1), 
                                                        # Evaluation metric
                                                        scoring = 'f1',
                                                        # Use all computer cores
                                                        n_jobs=-1, 
                                                        # 50 different sizes of the training set
                                                        train_sizes= train_sizess) 

# Create means and standard deviations of training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Create means and standard deviations of test set scores
validation_mean = np.mean(validation_scores, axis=1)
validation_std = np.std(validation_scores, axis=1)

import matplotlib.pyplot as plt
%matplotlib inline

plt.style.use('seaborn')
plt.plot(train_sizes, train_mean, label = 'Training error')
plt.plot(train_sizes, validation_mean, label = 'Validation error')

plt.ylabel('Score F1', fontsize = 14)
plt.xlabel('Training set size', fontsize = 14)
plt.title('Learning curves for final model', fontsize = 18, y = 1.03)
plt.legend()
plt.ylim(0.3,1.1)


#----------------------------------------------------------------------------- Evaluation visualization  #------------------------------------------------------------------





















################################################################################ utils for featuretools ################################################################################

######## utils for featuretools

import featuretools as ft
import pandas as pd
import numpy as np
import os

def load_entityset(data_dir):
    order_products = pd.read_csv(os.path.join(data_dir, "order_products__prior.csv"))
    orders = pd.read_csv(os.path.join(data_dir, "orders.csv"))
    departments = pd.read_csv(os.path.join(data_dir, "departments.csv"))
    products = pd.read_csv(os.path.join(data_dir, "products.csv"))

    order_products = order_products.merge(products).merge(departments)


    def add_time(df):
        df.reset_index(drop=True)
        df["order_time"] = np.nan
        days_since = df.columns.tolist().index("days_since_prior_order")
        hour_of_day = df.columns.tolist().index("order_hour_of_day")
        order_time = df.columns.tolist().index("order_time")

        df.iloc[0, order_time] = pd.Timestamp('Jan 1, 2015') +  pd.Timedelta(df.iloc[0, hour_of_day], "h")
        for i in range(1, df.shape[0]):
            df.iloc[i, order_time] = df.iloc[i-1, order_time] \
                                        + pd.Timedelta(df.iloc[i, days_since], "d") \
                                        + pd.Timedelta(df.iloc[i, hour_of_day], "h")

        to_drop = ["order_number", "order_dow", "order_hour_of_day", "days_since_prior_order", "eval_set"]
        df.drop(to_drop, axis=1, inplace=True)
        return df

    orders = orders.groupby("user_id").apply(add_time)
    order_products = order_products.merge(orders[["order_id", "order_time"]])
    order_products["order_product_id"] = order_products["order_id"].astype(str) + "_" + order_products["add_to_cart_order"].astype(str)
    order_products.drop(["product_id", "department_id", "add_to_cart_order"], axis=1, inplace=True)
    es = ft.EntitySet("instacart")


    es.entity_from_dataframe(entity_id="order_products",
                             dataframe=order_products,
                             index="order_product_id",
                                 variable_types={"aisle_id": ft.variable_types.Categorical, "reordered": ft.variable_types.Boolean},
                             time_index="order_time")

    es.entity_from_dataframe(entity_id="orders",
                             dataframe=orders,
                             index="order_id",
                             time_index="order_time")

    # es.entity_from_dataframe(entity_id="products",
    #                          dataframe=products,
    #                          index="product_id")

    es.add_relationship(ft.Relationship(es["orders"]["order_id"], es["order_products"]["order_id"]))
    # es.add_relationship(ft.Relationship(es["products"]["product_id"], es["order_products"]["order_id"]))

    es.normalize_entity(base_entity_id="orders", new_entity_id="users", index="user_id")
    es.add_last_time_indexes()

    # order_products["department"].value_counts().head(10).index.values.tolist()
    es["order_products"]["department"].interesting_values = ['produce', 'dairy eggs', 'snacks', 'beverages', 'frozen', 'pantry', 'bakery', 'canned goods', 'deli', 'dry goods pasta']
    es["order_products"]["product_name"].interesting_values = ['Banana', 'Bag of Organic Bananas', 'Organic Baby Spinach', 'Organic Strawberries', 'Organic Hass Avocado', 'Organic Avocado', 'Large Lemon', 'Limes', 'Strawberries', 'Organic Whole Milk']

    return es

def make_labels(es, training_window, cutoff_time,
                product_name, prediction_window):

    prediction_window_end = cutoff_time + prediction_window
    t_start = cutoff_time - training_window

    orders = es["orders"].df
    ops = es["order_products"].df

    training_data = ops[(ops["order_time"] <= cutoff_time) & (ops["order_time"] > t_start)]
    prediction_data = ops[(ops["order_time"] > cutoff_time) & (ops["order_time"] < prediction_window_end)]

    users_in_training = training_data.merge(orders)["user_id"].unique()

    valid_pred_data = prediction_data.merge(orders)
    valid_pred_data = valid_pred_data[valid_pred_data["user_id"].isin(users_in_training)]

    def bought_product(df):
        return (df["product_name"] == product_name).any()

    labels = valid_pred_data.groupby("user_id").apply(bought_product).reset_index()
    labels["cutoff_time"] = cutoff_time
    #  rename and reorder
    labels.columns = ["user_id", "label", "time",]
    labels = labels[["user_id", "time", "label"]]

    return labels

def dask_make_labels(es, **kwargs):
    label_times = make_labels(es, **kwargs)
    return label_times, es


def calculate_feature_matrix(label_times, features):
    label_times, es = label_times
    fm = ft.calculate_feature_matrix(features,
                                     entityset=es,
                                     cutoff_time = label_times,
                                     cutoff_time_in_index=True,
                                     verbose=False)

    X = merge_features_labels(fm, label_times)
    return X


def merge_features_labels(fm, labels):
    return fm.reset_index().merge(labels)

def feature_importances(model, features, n=10):
    importances = model.feature_importances_
    zipped = sorted(zip(features, importances), key=lambda x: -x[1])
    for i, f in enumerate(zipped[:n]):
        print("%d: Feature: %s, %.3f" % (i+1, f[0].get_name(), f[1]))

    return [f[0] for f in zipped[:n]]





